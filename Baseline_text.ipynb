{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "444356b2-23fc-49d0-b8eb-767a08049c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis import load_chat, load_highlights, remove_missing_matches, cut_same_length\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3c79fc-dca6-4237-bbd3-8097d4a2527d",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c28235da-ff5f-4474-a924-ec54dd9909c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_hl_to_documents(ch_match, hl_match, frames_per_doc=210, hl_threshold=0.5, offset=0):\n",
    "    # make non-overlapping windows of frames_per_doc/30 seconds\n",
    "    # TODO: implement offset to adjust for comment lag\n",
    "    chm = [ch_match[i:i+frames_per_doc] for i in range(0,len(ch_match),frames_per_doc)]\n",
    "    hlm = [hl_match[i:i+frames_per_doc] for i in range(0,len(hl_match),frames_per_doc)]\n",
    "    \n",
    "    chm_docs = list()\n",
    "    hlm_docs = list()\n",
    "    \n",
    "    for m_doc, h_doc in zip(chm, hlm):\n",
    "        chm_docs.append(re.sub(\"\\n+\", \" \", \"\".join(m_doc)).strip())\n",
    "        \n",
    "        # chm_docs.append(\"\".join(m_doc))\n",
    "        hlm_docs.append(int(sum(h_doc) >= hl_threshold)\n",
    "    \n",
    "    return chm_docs, hlm_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ded49d82-e355-48f4-bc80-e7f7eeec31e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_two_iters(a1, a2):\n",
    "    # shuffles two iterables in the same way and turns them into numpy arrays\n",
    "    shuffle_inds = list(range(len(a1)))\n",
    "    np.random.default_rng(42).shuffle(shuffle_inds)\n",
    "    \n",
    "    ret1 = np.asarray(a1)[shuffle_inds]\n",
    "    ret2 = np.asarray(a2)[shuffle_inds]\n",
    "    return ret1, ret2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c53a531f-9029-4d0f-8a81-3ebd02e0d1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_val_docs(mode=\"train\", shuffle=True, frames_per_doc=120, hl_threshold=0.5, offset=0):    \n",
    "    if mode == \"train\":\n",
    "        file_regex = \"nalcs_w[134579]*_g[13]\" # weeks 1,3,4,5,7,9 games 1 and 3 of each match for training\n",
    "    if mode == \"val\":\n",
    "        file_regex = \"nalcs_w[268]*_g[13]\" # weeks 2,6,8 games 1 and 3 of each match for validation\n",
    "\n",
    "    chat = load_chat(\"data/final_data\", load_random=5, random_state=42, file_identifier=file_regex)\n",
    "    highlights = load_highlights(\"data/gt\", file_identifier=file_regex)\n",
    "\n",
    "    remove_missing_matches(chat, highlights)\n",
    "\n",
    "    chat_docs = list()\n",
    "    hl_docs = list()\n",
    "\n",
    "    for match in chat.keys():\n",
    "        ch_match, hl_match = cut_same_length(chat[match], highlights[match])\n",
    "        cd, hd  = chat_hl_to_documents(ch_match, hl_match, frames_per_doc=frames_per_doc, hl_threshold=hl_threshold, offset=offset)\n",
    "        # put all documents together\n",
    "        chat_docs.extend(cd)\n",
    "        hl_docs.extend(hd)\n",
    "\n",
    "    # garbage collect chat and highlights    \n",
    "    \n",
    "    if shuffle:\n",
    "        chat_docs, hl_docs = shuffle_two_iters(chat_docs, hl_docs)\n",
    "        \n",
    "    return chat_docs, hl_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "8f3e509e-2e86-4d0e-8355-2dc068b4a388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing in highlights:\t {'nalcs_w5d2_C9_TMS_g1'}\n",
      "missing in chat:\t {'nalcs_w4d3_CLG_P1_g3', 'nalcs_w3d2_C9_NV_g1', 'nalcs_w5d3_FLY_TSM_g3', 'nalcs_w7d2_NV_TSM_g3', 'nalcs_w5d2_CLG_IMT_g1', 'nalcs_w1d3_NV_CLG_g1', 'nalcs_w4d2_TL_IMT_g1', 'nalcs_w9d2_TL_TSM_g3', 'nalcs_w5d2_NV_FOX_g1', 'nalcs_w5d3_FLY_TSM_g1', 'nalcs_w7d1_TL_C9_g1', 'nalcs_w3d3_DIG_CLG_g1', 'nalcs_w9d2_TL_TSM_g1', 'nalcs_w1d3_TL_FLY_g3', 'nalcs_w9d3_CLG_NV_g3', 'nalcs_w9d3_CLG_NV_g1', 'nalcs_w3d2_TL_FOX_g1', 'nalcs_w7d1_FLY_P1_g1', 'nalcs_w9d2_FOX_NV_g1', 'nalcs_w9d2_C9_P1_g1', 'nalcs_w3d2_CLG_TSM_g1', 'nalcs_w1d2_CLG_TL_g1', 'nalcs_w1d2_DIG_P1_g3', 'nalcs_w9d2_IMT_CLG_g1', 'nalcs_w7d3_FOX_DIG_g3', 'nalcs_w4d3_FLY_C9_g3', 'nalcs_w5d3_P1_C9_g1', 'nalcs_w4d2_P1_FLY_g1', 'nalcs_w3d3_IMT_FLY_g1', 'nalcs_w1d2_FOX_IMT_g3', 'nalcs_w4d1_C9_TL_g3', 'nalcs_w4d3_CLG_P1_g1', 'nalcs_w3d2_FLY_DIG_g1', 'nalcs_w7d2_P1_FOX_g1', 'nalcs_w7d3_TL_NV_g3', 'nalcs_w1d3_TSM_IMT_g1', 'nalcs_w3d1_NV_TL_g1', 'nalcs_w7d2_CLG_DIG_g1', 'nalcs_w7d3_TL_NV_g1', 'nalcs_w1d3_TL_FLY_g1', 'nalcs_w3d2_CLG_TSM_g3', 'nalcs_w9d3_FLY_TL_g1', 'nalcs_w9d3_FLY_TL_g3', 'nalcs_w4d3_FOX_TSM_g1', 'nalcs_w5d3_IMT_FOX_g1', 'nalcs_w1d2_DIG_P1_g1', 'nalcs_w5d2_TL_DIG_g1', 'nalcs_w1d1_TSM_C9_g1', 'nalcs_w5d1_NV_P1_g1', 'nalcs_w5d2_CLG_IMT_g3', 'nalcs_w4d2_CLG_FOX_g1', 'nalcs_w5d2_TL_DIG_g3', 'nalcs_w9d2_FOX_NV_g3', 'nalcs_w5d2_C9_TSM_g1', 'nalcs_w1d3_DIG_C9_g3', 'nalcs_w9d3_DIG_IMT_g1', 'nalcs_w5d2_C9_TSM_g3', 'nalcs_w4d3_FLY_C9_g1', 'nalcs_w9d1_FLY_FOX_g1', 'nalcs_w7d3_CLG_C9_g3', 'nalcs_w3d3_C9_FOX_g1', 'nalcs_w4d2_TSM_NV_g1', 'nalcs_w4d1_IMT_DIG_g1', 'nalcs_w1d2_NV_FLY_g1', 'nalcs_w3d3_P1_TSM_g1', 'nalcs_w1d3_TSM_IMT_g3', 'nalcs_w3d2_TL_FOX_g3', 'nalcs_w9d3_TSM_P1_g1', 'nalcs_w1d1_FOX_P1_g1', 'nalcs_w7d3_CLG_C9_g1', 'nalcs_w7d3_FOX_DIG_g1', 'nalcs_w7d2_NV_TSM_g1', 'nalcs_w1d2_FOX_IMT_g1', 'nalcs_w5d3_TL_CLG_g3', 'nalcs_w1d3_DIG_C9_g1', 'nalcs_w9d1_C9_DIG_g1', 'nalcs_w4d1_C9_TL_g1', 'nalcs_w4d3_DIG_NV_g1', 'nalcs_w7d3_IMT_TSM_g1', 'nalcs_w7d3_IMT_TSM_g3', 'nalcs_w3d3_P1_TSM_g3'}\n"
     ]
    }
   ],
   "source": [
    "ch_train, hl_train = load_train_val_docs(mode=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db64a17-5b46-490b-b618-82b806d9037a",
   "metadata": {},
   "source": [
    "### Oversampling for balancing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b98a319a-dd01-464b-a447-1cb4bad29acc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>total_train_samples</th>\n",
       "      <th>num_highlight_samples</th>\n",
       "      <th>num_non_highlight_samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>before_over_sampling</td>\n",
       "      <td>2768</td>\n",
       "      <td>355</td>\n",
       "      <td>2413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   step  total_train_samples  num_highlight_samples  \\\n",
       "0  before_over_sampling                 2768                    355   \n",
       "\n",
       "   num_non_highlight_samples  \n",
       "0                       2413  "
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# over-sample\n",
    "ch_pos = ch_train[hl_train == 1]\n",
    "hl_pos = hl_train[hl_train == 1]\n",
    "ch_neg = ch_train[hl_train == 0]\n",
    "hl_neg = hl_train[hl_train == 0]\n",
    "\n",
    "df_sample_numbers = pd.DataFrame({\"step\": \"before_over_sampling\", \"total_train_samples\": [len(hl_train)], \"num_highlight_samples\": [len(hl_pos)],  \"num_non_highlight_samples\": [len(hl_neg)]})\n",
    "df_sample_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "60268444-2113-492e-a5f6-bb5931b18581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>total_train_samples</th>\n",
       "      <th>num_highlight_samples</th>\n",
       "      <th>num_non_highlight_samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>before_over_sampling</td>\n",
       "      <td>2768</td>\n",
       "      <td>355</td>\n",
       "      <td>2413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>after_over_sampling</td>\n",
       "      <td>4543</td>\n",
       "      <td>2130</td>\n",
       "      <td>2413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   step  total_train_samples  num_highlight_samples  \\\n",
       "0  before_over_sampling                 2768                    355   \n",
       "1   after_over_sampling                 4543                   2130   \n",
       "\n",
       "   num_non_highlight_samples  \n",
       "0                       2413  \n",
       "1                       2413  "
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hl_pos_rep = np.repeat(hl_pos, int(len(hl_neg) / len(hl_pos)))\n",
    "ch_pos_rep = np.repeat(ch_pos, int(len(hl_neg) / len(hl_pos)))\n",
    "\n",
    "hl_train_balanced = np.concatenate([hl_pos_rep, hl_neg])\n",
    "ch_train_balanced = np.concatenate([ch_pos_rep, ch_neg])\n",
    "\n",
    "ch_train_balanced_shuffled, hl_train_balanced_shuffled = shuffle_two_iters(ch_train_balanced, hl_train_balanced)\n",
    "\n",
    "df_sample_numbers = df_sample_numbers.append({\"step\": \"after_over_sampling\", \"total_train_samples\": len(hl_train_balanced_shuffled), \"num_highlight_samples\": len(hl_pos_rep),  \"num_non_highlight_samples\": len(hl_neg)}, ignore_index=True)\n",
    "df_sample_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e13eaf6f-f126-4150-a5af-f522184edb7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['FeelsBad YOUR TIME TO SHINE FeelsBadMan WHAT HAPPENED TO FLYQUEST fly feelsbad FREEQUEST LUL CLOWN FIESTA KreyGasm easy lmao DIG PogChamp NA SO BAD 4Head j4 mid no more Jebaited DIGNITAS DIGNITAS PogChamp DIG PogChamp PogChamp PogChamp PogChamp gg POG lose vs dig = disband LUL DIG PogChamp WutFace FAILED QUEST LUL fly trolling DIG PogChamp NV LUL PLOWED haHAA m DIG PogChamp (poolparty) WutFace nv 9>2 Kappa DIG PogChamp',\n",
       "       'QUEM É BRASIL, DIGITE 1 NICOLE ANISTON Kreygasm LUL LUL WTF WTF LOLOL balls? SSUMDAY PogChamp sumday 1 v9 FLYQUEST LL ??????? ????? HELLO??!? WutFace ???? ???? DIG PogChamp altec? kleane Kreygasm PogChamp team? KARTHUS YOOOO DOG keaneW keaneW keaneW keaneW keaneW LUL triple TAUNT Keane PogChamp ????? Throw LUL DIG PogChamp FLY????????? KEANE PogChamp ??????? BALLS LUL FIESTA BALLS ? KEANE PogChamp DIG IN 2017 PogChamp PogChamp LUL DIG PogChamp DIG PogChamp DIG PogChamp report balls LUL D2 flyquest wtf PogChamp PogChamp PogChamp PogChamp PogChamp PogChamp ?',\n",
       "       '4Head CODY !!! CODY WON CODY DID IT Fake PogChamp CODY WON 2-1 FLY INC P PogChamp B HAI LUL CODY FUN SO UGLY P PogChamp B LOLOLO POB FLAME HORIZON PogChamp BibleThump pobChamp pobChamp pobChamp pobChamp pobChamp pobChamp',\n",
       "       \"Zed LUL 🤔 PogChamp FLY? ??????? ??????????? REKT PogChamp BALLS!!!!! FIESTA VisLaud SHEN CARRY PogChamp FeelsBadMan I'M NOT BETTER THAN ANYONE FeelsBadMan PogChamp BALLS LUL BALLS IS ACTUALLY TRASH, HE COULD HAVE KILLED THOSE 3 PPL PogChamp balls PogChamp LUL DIG PogChamp ! TEAM??????? NA-CLOWNFIESTA LOL ????????/ LUL D2 D2 FIGHTING GRAVES IN A CHOKE POINT LUL HOLY *** PogChamp DIG PogChamp DIG PogChamp LOL LOL ???\",\n",
       "       '@ZionsFutureWife HeyGuys yo',\n",
       "       'LUL NOW LOL PogChamp PogChamp NOW PogChamp PogChamp PogChamp SMOrc REMOVE KEBAB SMOrc PogChamp L U L NO MOre ANELE REKT PogChamp RYU PogChamp HotPokket behave chat HotPokket 4Head at 44:44 NAO RYU PogChamp PogChamp',\n",
       "       '11 GG ADRIAN LUL FREE TYLER1 BigBrother SMITE KING LUL DARDUMB 11 NotLikeThis LUL LL 4 barons LUL RUINED LUL VARUS Q LUL',\n",
       "       'GG GG EZ NO RE Kappa Kappa Kappa kappa envy lul GG EZ PZ C9 Jebaited PogChamp PogChamp DIG STOP SCREAMING COP LOL FeelsBadMan MY TIME FeelsBadMan FREE QUEST 4Head 4Head 4Head zero ult combos LUL NV LUL gg ez Imagine if they had taric support PogChamp ez LUL Plow SwiftRage gg FLYQUEST NotLikeThis DIG COACH COP COP PogChamp PLOW 4Heas wait what VIRTUS PLOW PogChamp NV LUL diddly doger wtf FREEQuest FREEQUEST LUL DIG otofuMacho otofuMacho PLOWED COP CRAFT PogChamp COP A GOD DIG PogChamp FallQuest LUL',\n",
       "       'tyler1LUL Fly actually looking like worst place team lol the *** flyquest 0 dmge LOSEQUEST LUL',\n",
       "       'LUL SMOrc KILL REIGNOVER SMOrc ftnaOK ftnaOK ftnaOK SMOrc SUB IN CHAUSTER jhin doing baron LUL'],\n",
       "      dtype='<U792')"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ch_train[hl_train == 1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "b174bf25-69c3-4060-8e58-8835c3dc2596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4543, 6608)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer() # TODO add parameters\n",
    "X_train_counts = count_vect.fit_transform(ch_train_balanced_shuffled)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "a8a3e499-d05c-4d2f-95d8-e4077a13c170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4541"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.vocabulary_.get(u'pogchamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "4af697ba-98e2-432d-b8cf-06cf405bbda2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4543, 6608)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "88fcab4b-4098-4d98-a2f7-e105225cd3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = \n",
    "y_train = hl_train_balanced_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "2d3531ab-cc7e-4657-b35a-3afbb01f68e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "9dd6f9d9-6837-47ab-993c-54b56dbbeeb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing in highlights:\t {'nalcs_w6d2_FOX_C9_g1'}\n",
      "missing in chat:\t {'nalcs_w2d3_FOX_FLY_g3', 'nalcs_w8d2_TSM_FLY_g1', 'nalcs_w2d2_IMT_C9_g1', 'nalcs_w8d2_NV_DIG_g1', 'nalcs_w8d2_C9_IMT_g3', 'nalcs_w2d2_TSM_TL_g3', 'nalcs_w6d1_IMT_TL_g1', 'nalcs_w8d2_NV_DIG_g3', 'nalcs_w2d2_DIG_FOX_g3', 'nalcs_w8d2_TL_P1_g1', 'nalcs_w6d1_P1_CLG_g3', 'nalcs_w2d3_C9_CLG_g3', 'nalcs_w2d1_TSM_DIG_g1', 'nalcs_w6d1_P1_CLG_g1', 'nalcs_w2d3_C9_CLG_g1', 'nalcs_w8d2_C9_IMT_g1', 'nalcs_w2d3_NV_IMT_g3', 'nalcs_w8d2_TSM_FLY_g3', 'nalcs_w6d2_FLY_NV_g3', 'nalcs_w2d2_P1_NV_g1', 'nalcs_w2d2_DIG_FOX_g1', 'nalcs_w2d1_FLY_CLG_g1', 'nalcs_w2d1_TSM_DIG_g3', 'nalcs_w6d3_C9_FLY_g1', 'nalcs_w8d1_DIG_TL_g1', 'nalcs_w8d1_FOX_CLG_g1', 'nalcs_w6d2_TSM_CLG_g1', 'nalcs_w2d2_TSM_TL_g1', 'nalcs_w6d3_FOX_TL_g3', 'nalcs_w8d3_CLG_FLY_g1', 'nalcs_w8d3_P1_IMT_g1', 'nalcs_w2d3_P1_TL_g1', 'nalcs_w8d3_NV_C9_g1', 'nalcs_w6d2_P1_DIG_g3', 'nalcs_w6d1_IMT_TL_g3', 'nalcs_w8d3_TSM_FOX_g1', 'nalcs_w6d3_DIG_TSM_g1', 'nalcs_w2d3_FOX_FLY_g1', 'nalcs_w6d3_FOX_TL_g1'}\n"
     ]
    }
   ],
   "source": [
    "# load validation data\n",
    "ch_val, hl_val = load_train_val_docs(mode=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "c830fed5-c348-42ea-bebd-3d2589c749d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = hl_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "09be2811-eef6-4704-8a5a-4430255a4468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.2006079027355623, 0.3656509695290859, 0.25907752698724235, None)\n",
      "0.7488356620093147\n"
     ]
    }
   ],
   "source": [
    "docs_new = ch_val\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "y_pred = clf.predict(X_new_tfidf)\n",
    "\n",
    "\"\"\"\n",
    "for doc, pred, true in zip(docs_new, y_pred, y_true):\n",
    "        if pred == true == 1:\n",
    "             print(doc, pred, true)\n",
    "\"\"\"\n",
    "print(precision_recall_fscore_support(y_true, y_pred, average=\"binary\"))\n",
    "print(accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5f8dce-8050-4590-807b-f12f25c170b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0401d299-a77e-411c-bb1d-8f8085a334df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c753bff6-c17c-49c9-8fd8-d8c12e385487",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
